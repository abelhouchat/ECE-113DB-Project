\documentclass[journal]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{courier}
\usepackage{cuted}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx}
\usepackage{cleveref}

\lstset{basicstyle = \ttfamily, breaklines = true}

\begin{document}

\title{Real-time Chord Recognition}
\author{Adam Belhouchat, Ashish Sareen}
\maketitle

\begin{abstract}
    This project describes the development of a real-time chord recognition system, which was built using the TI LCDK embedded chip and Code Composer Studio IDE \cite{lcdk, ccs}.
    Prior chord recognition systems have been put in place, but our implementation is specifically intended for use by a musician during the real-time composition process \cite{stark, cho, harte, lee, mauch_thesis, mauch_simultaneous, jiang, pauwels, fujishima}.
    The subsystems are, in order: recording audio of live chord playing, extracting chroma features, performing chord recognition, and providing output to the user.
    The output is both in real-time to the computer’s console and to an annotated text file after the recording is complete.
    Our results indicate that as long as the chord is played in time to the program (with live prompts to the user), the system works as intended with minor undesirable effects.
\end{abstract}
\begin{IEEEkeywords}
    chord, octave, chord recognition, chroma features, pitch class profile, pattern matching, cosine similarity, filter, downsampling, FFT, LCDK
\end{IEEEkeywords}

\section{Introduction}
\subsection{History with References}
Automatic chord recognition (ACR) is the process of determining what chord is being played given an audio sample, labelling the audio sample over time with the appropriate chord \cite{stark}.
A chord here is defined as two or more notes being played at the same time or close together \cite{cho}.
Chord recognition has many applications, such as music segmentation or determining the similarity between two music samples \cite{lee}, but one of the most attractive applications is in automatic transcription.
Manual transcription can be tedious and difficult, and automatic chord recognition systems can help musicians work more quickly and more accurately \cite{mauch_thesis}.
Because of its wide range of applications, chord recognition has been the subject of much research in the past few decades, trying to improve the number of chords it can recognize and its accuracy through novel methods and approaches \cite{pauwels}.

One of the first ACR systems was developed by Takuya Fujishima in 1999 \cite{pauwels}.
He used the discrete Fourier transform (DFT) of a music sample to generate a pitch class profile, also known as chroma feature or chroma vector, which encodes the harmonic information of the sample over twelve pitch classes.
The chroma feature is then compared the templates, one for each of 27 different chords, and the closest match is chosen as the chord label in a process called pattern matching \cite{fujishima}.

Since then, chord recognition has evolved greatly.
Pattern matching is simple and effective, but it is not the most accurate \cite{jiang}.
Some researchers have found great success using hidden Markov models (HMMs) rather than template matching to perform ACR \cite{sheh}, and others have explored the use of deep learning and neural networks both to detect chords \cite{boulanger} and to extract features from the audio sample \cite{korzeniowski}.
However, both HMMs and deep learning require a significant amount of training data, and good training data may be difficult to come by.
Manual transcription is prone to errors due to human subjectivity \cite{pauwels} and as mentioned before, it can be time consuming.
Nevertheless, these approaches do show significant accuracy improvements over pattern matching \cite{jiang, boulanger}.
Each of these approaches has its pros and cons, and which one is most fitting depends on the specific application.


\subsection{Global Constraints}
The main constraints dealt with are the hardware limitations of the TI LCDK platform, which is an all-in-one microcontroller (MCU) with digital signal processor (DSP) and ARM processor (CPU), and other common peripheral components \cite{lcdk}.
The entire system is built around the operation of the LCDK unit.
The two major limitations are the memory size and processor speed.
The memory specifications are 128 MB SDRAM and 128 MB NAND Flash \cite{lcdk}.
The processor speed is listed as 456 MHz for both the DSP and CPU \cite{lcdk}.

These hardware constraints pose implications on the software needed to build the system.
The LCDK board is programmed using the TI Code Composer Studio (CCS) IDE in the C programming language \cite{lcdk}.
The memory constraints imply that the program’s memory allocation must be limited to prevent data overflow.
To that end, memory is dynamically allocated when needed and freed immediately after use with the in-built C \texttt{malloc()} and \texttt{free()} functions.
Since this is a real-time system, the processor speed places a constraint on the execution time of the program.
The system leverages the interrupt service routine (ISR) to take audio samples at the sample rate equal to 16000 Hz.
The interrupt loop in the code must have an execution time less than 62.5 microseconds in order to guarantee that samples are taken properly.
Functions that take a long time to complete, such as \texttt{fprintf()}, are used only at the end of the program when the real-time component is complete.


\section{Motivation}
One shortcoming of many modern chord recognition systems is the vast amount of data required to train these systems \cite{pauwels}.
As mentioned earlier, large datasets of accurately transcribed music are not easy to find, so we wanted to develop a system that would work without training.
Additionally, these data-driven approaches are opaque and it is difficult to understand how they determine the chords in an audio sample \cite{pauwels}.
Part of our motivation for pursuing this project was to increase our skills in real-time digital signal processing and to understand how automatic chord recognition works.
Throwing data at a neural network until it works does not help us understand chord recognition.

Another shortcoming of HMMs and neural networks is they are often more computationally expensive than pattern matching \cite{stark}.
They work well when a music sample has been prerecorded and is later fed into the system, but they are not fast enough to perform chord recognition and transcription in real-time.
We wanted to make a chord recognition system that would record musical chords and transcribe them in real-time, so we needed to approach our system differently from most modern systems.

Lastly, as mentioned earlier, we chose this project to learn more about real-time digital signal processing (DSP) and to gain more experiences with different applications of it.
Chord recognition requires many DSP techniques, including sampling a signal, processing it to extract features, performing some evaluation on those features, and then returning the results of that evaluation to the user \cite{fujishima}.
This project would give us valuable experience with all the important elements of DSP with the added constraint of performing it in real-time.
This required us to perform memory management and pay close attention to our code to make sure all functions were optimized, which are valuable skills for embedded software development.


\section{Approach}
\subsection{Team Organization}
The team for the project consisted of two people, and the majority of the work was done together in the lab during lab time.
Having two people worked well, because we could split up the work into its natural software and hardware components.
A typical workflow would have one person controlling the software of the LCDK with CCS and the other monitoring the interfaces, such as microphone and speakers, to the LCDK.
The software person would write code and run it on the LCDK.
The hardware person would be responsible for testing the program by giving the microphone input through recorded or live chord sequences.
Being able to incrementally test units in this fashion enabled us to quickly develop and debug our code.
We were generally successful at managing our time, utilizing the full extent of our given lab time and not needing to come in after hours.
With proper time management and team organization, we were able to accomplish our main goals.


\subsection{Plan and Implementation}
Our initial plan was as follows:

\subsubsection*{Weeks 1--2}
Find and format data.
Find a data set of various guitar chords.
Format data into frames.

\subsubsection*{Weeks 3--4}
Generate chroma features.
Test feature extraction in MATLAB.
Implement final version of feature extraction on LCDK.

\subsubsection*{Weeks 5--7}
Perform chord recognition on chroma features.
Test different classification algorithms (template, HMM, linear classifier, etc.) in MATLAB and find the best (most accurate) one.
Implement final classification algorithm on LCDK.

\subsubsection*{Weeks 8--9}
Test chord recognition (all on LCDK).
Test on data set.
Test on live performance.
Write output to console.

\subsubsection*{Weeks 9--10}
Write chord sequences to a file.
Write a plain text file for testing.
Write to standard format (guitar tabs, sheet music, etc.).

Our actual implementation went as follows:

\subsubsection*{Weeks 1--2}
We found several potential data sets from which samples were taken.
Then, we were able to construct a data set with several audio samples for each of the 24 chords that were implemented.

\subsubsection*{Weeks 3--4}
Instead of MATLAB, we used Python to prototype chroma feature extraction.
Python had an up-to-date library for chroma processing, and was easier to use than MATLAB \cite{librosa}.
Referring to this library and another paper, we were able to implement feature extraction on the LCDK \cite{librosa, stark}.

\subsubsection*{Weeks 5--7}
Instead of beginning with Python or MATLAB, we started developing on the LCDK to save time.
Our first attempt was to adapt a template-matching algorithm, which ended up working very well \cite{stark}.
It was the simplest to implement and had the lowest computational complexity.
We ended up not having to attempt any of the other classifiers, which included linear classifiers, HMM, and SVM.

\subsubsection*{Weeks 8--9}
We tested chord recognition by playing sequences of guitar chords and recording the classification accuracy.
We were pleased to find that when played live, the program was able to classify all of the chords.

\subsubsection*{Weeks 9--10}
We were able to write the sequences to a text file by assigning timestamps for the duration that a chord was played.
We did not get to our stretch goal, which was to write the sequence to a standard format.


\subsection{Standard}
The most common features used in automatic chord recognition systems are chroma features \cite{cho_chroma}.
First introduced by Fujishima, chroma features represent the pitch content of a music sample.
Basic chroma features are calculated by summing the magnitude of the DFT of the music signal over certain frequency bins \cite{fujishima}.
These bins correspond roughly to the energy in different pitch classes, such as C or G\#.
Each chroma vector has twelve elements, one for each note in an octave, corresponding to the twelve pitch classes that the frequency bins are mapped to, and the distribution of energy across the twelve pitch classes is used to determine which chord was played in the music sample \cite{jiang}.
Because of how ubiquitous and useful chroma features are in chord recognition, we chose to use them as our features for chord recognition.

To actually determine what chord the chroma features corresponds to, we use pattern matching, which is one of the most common methods of determining chords \cite{cho_chroma}.
In particular, we use binary pattern matching, where each chord has a corresponding template which is a specific arrangement of zeros and ones.
Since the chroma vector represents the pitch content of a music sample, we can create ideal template vectors that represent what the pitch content of a pure chord would look like.
For example, the template vector for the major C chord would be $\mathbf{v}_C = (1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0)$, where the ones are where a major C should have pitch contributions and the zeros are pitches that should not be present in a major C.
In general, the tones that should be present in the ideal chord are one, while all the other tones are zero \cite{cho_chroma}.
To find the matching template, many implementations look for the minimum Euclidean distance between a template and the chroma feature \cite{stark}, but other approaches such as finding the minimum angle between the two are also used \cite{jiang}.
We chose to go with this latter approach as it gave us slightly better accuracy when testing.


\subsection{Theory}
The key mathematical elements of our chord recognition system are the low-pass filter, the chroma features, and pattern matching.
In this section, we will explain in detail the theory behind each of these steps.

The low-pass filter is generated using the bilinear transform, which converts a continuous transfer function to a discrete one.
To do so, the bilinear transform uses the approximate map
\begin{equation}
    s \to \frac{2}{T}\frac{1 - z^{-1}}{1 + z^{-1}}
    \label{eq:s_to_z}
\end{equation}
where T is the sampling period of the discrete signal.
We can then substitute this into the continuous transfer function $H_a(s)$ to get the discrete one $H_d(z)$ by 
\begin{equation}
    H_d(z) = H_a\!\left(\frac{2}{T}\frac{1 - z^{-1}}{1 + z^{-1}}\right)
    \label{eq:bilinear}
\end{equation}
Once we have $H_d(z)$, we can get the discrete time-domain equation by partial fractions and inverse Z-transform pairs.

We used a 6th order Butterworth low-pass filter with a cutoff frequency at 6000 Hz.
This cutoff frequency preserved most of the important harmonic information while still filtering out high-frequency noise, and the order gave a reasonably steep dropoff around the cutoff frequency without being computationally expensive.
The actual transfer function is too large and complex to show here, so we used a program to generate the time-domain filter function.
The frequency response is given in \Cref{fig:frequency_response}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../Figures/frequency_response}
    \caption{Frequency response of the low-pass filter used in our chord recognition system.
    The red curve is the normalized magnitude of the frequency response and the blue curve is the phase.
    Generated from Tony Fisher's website at \url{https://www-users.cs.york.ac.uk/~fisher/mkfilter/}.}
    \label{fig:frequency_response}
\end{figure}

Chroma features are features that describe the pitch content of a sample of music \cite{jiang}.
A chroma feature has 12 elements, one for each note in an octave from C through B.
The value of one of these elements corresponds to the energy present in the music signal coming from that pitch.
Because of this, it is straightforward to determine the chords in a piece of music by analyzing the chroma features.

The algorithm we use to generate chroma features comes from Stark and Plumbley’s paper \cite{stark}.
The first step is to apply a Hamming window $w(n)$ to the signal $x(n)$, given by
\begin{equation}
    w(n) = 0.54 - 0.46\cos\!\left(\frac{2\pi n}{N - 1}\right)
    \label{eq:window}
\end{equation}
where $N$ is the size of the signal.
Once we have the windowed signal $x_w(n) = x(n)w(n)$ we take the DFT,
\begin{equation}
    X(k) = \sum_{n = 0}^{N - 1}x_w(n)e^{-\frac{i2\pi kn}{N}}
    \label{eq:dft}
\end{equation}

From here, the original chroma feature algorithm would sum the squared magnitudes of the DFT over certain frequency bins to get the chroma values \cite{fujishima}.
However, this may include unwanted energy like noise in the chroma features.
To avoid this, Stark and Plumbley only consider the maximum amplitude in a given frequency bin, thus only taking into account the energy in the note we want.

First, we determine the frequencies to search over.
We start with $f_{\text{C}3} = 130.81$ Hz, which is lower C.
Then for $n = 0, 1, \ldots, 11$, we calculate
\begin{equation}
    f(n) = f_{\text{C}3}2^{(n / 12)}
    \label{eq:freq}
\end{equation}
which gives the 12 notes in an octave.
We then do this for two octaves so we get all 24 notes in the two octaves from $f_{\text{C}3} = 130.81$ Hz to $f_{\text{C}5} = 523.25$ Hz.
For each of the 12 elements of the chroma feature, we search through two octaves and within each octave search through two harmonics.
This is because Stark and Plumbley have found that most instruments of interest use the lower register within this frequency range and to account for inharmonicities in real instruments \cite{stark}.
The chroma feature is then given by
\begin{equation}
    c_n = \frac{1}{h}\sum_{\phi = 1}^2\sum_{h = 1}^2 \max_{k_0^{(n, \phi, h)} \leq k \leq k_1^{(n, \phi, h)}} X(k)
    \label{eq:chroma}
\end{equation}
where $c_n$ are the elements of the chroma vector $\mathbf{c}$, $n = 0, 1, \ldots, 11$, $\phi$ is the number of the octave, $h$ is the number of the harmonic, and 
\begin{equation*}
    k_0^{(n, \phi, h)} = k'^{(n, \phi, h)} - rh
\end{equation*}
\begin{equation*}
    k_1^{(n, \phi, h)} = k'^{(n, \phi, h)} + rh
\end{equation*}
where $r = 2$ is the number of bins to search over for each harmonic and 
\begin{equation*}
    k'^{(n, \phi, h)} = \text{round}\!\left(\frac{f(n)\phi h}{f_s / N}\right)
\end{equation*}
where $f_s$ is the sampling rate.

Lastly, we use pattern matching to match the chroma feature to the most fitting chord.
Pattern matching involves the use of template vectors for each chord, such as $\mathbf{v}_{\text{C}} = (1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0)$ for major C \cite{jiang}.
We try to match the chroma vector to the closest template vector and from that get the corresponding chord.
One simple method is to just find the Euclidean distance $||\mathbf{c} - \mathbf{v}||$ between the chroma vector and each of the template vectors and see which is the smallest.
Another approach, which we use,  is to find the cosine similarity between two vectors, defined as 
\begin{equation}
    \text{cossim}(\mathbf{c}, \mathbf{v}) = \frac{\langle\mathbf{c}, \mathbf{v}\rangle}{||\mathbf{c}|| \cdot ||\mathbf{v}||}
    \label{eq:cossim}
\end{equation}
and choose the template vector with the maximum cosine similarity \cite{jiang}.


\subsection{Software/Hardware}
There were two main types of software used for the project: the LibROSA Python package for prototyping, and Code Composer Studio (CCS) C software for implementation \cite{librosa, ccs}.
The LibROSA Python package was used in the early stages of development for chroma feature extraction.
The main motivation was to familiarize ourselves with the chroma structure and the type of inputs/outputs that our program would need.
The LibROSA package also provided functions to visualize the contents of a chroma feature, which helped to further understand the nature of the audio transformation \cite{librosa}.
After using this package, we were able to better organize our C code for the LCDK, keeping in mind its real-time constraints.

The majority of the software was done in Code Composer Studio in order to program the LCDK.
We were familiar with CCS since we used it extensively in prior projects.
The main challenge was to adapt the chroma feature extraction and chord recognition into the C programming language, which requires more low-level knowledge than Python.
Fortunately, with prior experience in C/C++ and relative simplicity of the algorithms used, we were able to quickly implement our program.
An example code snippet from the chroma feature extraction, containing the \texttt{process\_audio\_frame()} function, found in \texttt{chromagram.h}, is provided in \Cref{code:audio} at the end of this report.

As aforementioned, the LCDK platform is the embedded system on which the compiled C program operates \cite{lcdk}.
The main subsystems used were the CPU and DSP, memory, and audio codec.
The audio codec was easy to use, since the manufacturer provided the encapsulated functions needed to sample and save audio information to the C built-in data types.
Encapsulation enabled us to use the hardware without needing to know many of its low-level details and instead use our knowledge of the software.


\subsection{Operation}


\subsubsection{How the System was Built}
We broke our chord recognition system into three major parts based on the three major components we needed to implement: chroma feature extraction, chord recognition, and chord sequence output.
The block diagram/flowchart outlining our system is given in \Cref{fig:block}.
\begin{figure}[!t]
    \centering
    \includegraphics[width = \linewidth]{../Figures/block_diagram_2}
    \caption{Block diagram flowchart of the system.
    There are three main steps: chroma feature extraction, chord recognition, and chord sequence output.}
    \label{fig:block}
\end{figure}

The first block we worked on, and the first component that is used in chord recognition, is chroma feature extraction.
We first decided to record our audio through a microphone (so we could perform live chord recognition) at a sampling rate of 16000 Hz and to record 4096 samples.
This would give us roughly a quarter-second of audio, long enough to achieve good resolution for the chroma features, and the sampling rate was high enough that we could capture important high frequency components to the audio.
We also needed the number of samples to be a power of 2 for the FFT and chroma feature calculations.
As the microphone was not very high quality, we needed to implement a low-pass filter to remove high frequency noise from the input.
Our reasoning for choosing a 6th order Butterworth filter with cutoff at 6000 Hz was given in the Theory section.

After low-pass filtering the recorded audio, we needed to downsample it by a factor of 4, which reduces the size of the audio sample so that subsequent calculations do not take too long.
As we want to perform real-time chord recognition, we need the chroma feature calculations to be fast enough that they do not significantly slow down the program's speed.
We did attempt to bypass the downsampling step by recording at a lower sampling rate, but this gave very poor results and so we decided to stick with downsampling.

After downsampling the filtered audio signal, we calculate the chroma feature through the algorithm described in the Theory section.
Examples of the chroma features are given in \Cref{fig:a_major,fig:a_minor} for the A major and A minor chords respectively played on a guitar.
\begin{figure}[!t]
    \centering
    \includegraphics[width = \linewidth]{../Figures/chromagram_a_major}
    \caption{Chromagram for the A major chord.
    The distortion at the beginning is due to the initial strum of the guitar.
    Generated with LibROSA using Python.}
    \label{fig:a_major}
\end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width = \linewidth]{../Figures/chromagram_a_minor}
    \caption{Chromagram for the A minor chord.
    The distortions at the beginning and end are due to the initial strum of the guitar and the audio trailing off.
    Note how two out of the three most prominent notes are the same as for the A major chord.
    Generated with LibROSA using Python.}
    \label{fig:a_minor}
\end{figure}
For each frame of recorded audio (4096 samples), we generate one chroma vector.
Due to the limited memory of the LCDK, we cannot store very many frames of audio and their chroma features at once, so we run our pipeline one at a time, recording a single frame and generating a single chroma vector and performing chord recognition on that chroma vector before moving on to the next frame of audio.

After we have generated the chroma feature, we move on to chord recognition.
We first create the templates for each chord, and we also normalize their magnitudes in order to make comparison easier.
After generating the templates, we calculate the cosine similarities between the chroma feature and each of the templates and find the maximum cosine similarity and choose the corresponding chord as the detected chord.

Lastly, we output the detected chord.
We first write the chord to the CCS console in real-time in order to provide immediate feedback to the user about which chord is being played.
We also wanted to write the chord to a final output file with timestamps of when the chord was played, so we decided to keep track of the chords detected during each time frame by storing them in an array.
However, because of this, we now had to limit the runtime of the program to a fixed time.
If we were to let the program run indefinitely, we would run into memory issues with a continuously growing and reallocating array, so we decided to force the program to terminate after a fixed number of loops.
Therefore, once the program completes the given number of iterations, it processes the final chord sequence to assign timestamps, using the sampling rate and frame size to estimate the length of each loop, and writes the timestamped sequence of chords to a text file.

To make this process easier on the user, we decided after implementing all three blocks to play a ``metronome'' through speakers connected to the output port of the LCDK.
During our initial testing, we discovered that the accuracy of the chord recognition system was highly dependent on whether the chord was played in time with the program's recording loop.
If the chord was played at the same time as the LCDK started recording, we would get good performance, but if the two were out of sync then we would have very poor performance.
To fix this, our metronome tells the user how to time their audio to achieve good performance.
At every loop, the beat plays once the LCDK begins recording the audio frame so that the user can play their audio in time with the recording.



\subsubsection{How to Use the System}
To use the chord recognition system, first plug a microphone into the LCDK microphone input port and plug speakers into the LCDK output port.
If you want, you can change the sampling rate, frame size, buffer size, and runtime.
The first three parameters are defined in \texttt{chromagram.h}: \texttt{FS} is sampling rate, \texttt{FRAME\_SIZE} is frame size, and \texttt{BUFFER\_SIZE} is buffer size which is fed into the FFT.
Note that \texttt{FRAME\_SIZE} must be a power of 2 and \texttt{BUFFER\_SIZE} must be one-fourth of \texttt{FRAME\_SIZE} due to the downsampling.
We choose to define them separately just to save that little bit of processing power required to perform the division by four.
The program length is implicitly defined by \texttt{ITERATIONS} in \texttt{main.c}.
Strictly speaking, it controls how many loops the program will run for, which is also the number of chords that will be processed.

Once all the parameters are set to your liking, run the program through Code Composer Studio and you will hear the “metronome” start to play through the speakers.
The metronome is really just a square wave that plays on beat to when the recording starts.
Once you hear the metronome, you can begin playing your audio samples into the microphone.
For best results, play the chords in time with the metronome, because otherwise the chord will be out of sync with the recording and the performance of the chord recognition system will be poor.

Once you start the program, the system will write the chords it has recognized to console.
The most recent message will be the chord that was just played and recorded.
Once the program has run through all iterations, it will print ``Done :)'' at which point you will find a file named \texttt{chord\_sequence.txt} in the Debug folder.
This prints out all the chords that were played during recording along with the times that the chord was played.


\section{Results}

\begin{figure}[!t]
    \centering
    \includegraphics[width = \linewidth]{../Figures/chord_sequence_in_order}
    \caption{Graph of the performance of the system for a naturally ordered chord progression.}
    \label{fig:in_order}
\end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width = \linewidth]{../Figures/chord_sequence_random}
    \caption{Graph of the performance of the system for a randomly ordered chord progression.}
    \label{fig:random}
\end{figure}


\subsection{Description}
We graphed the performance of our chord recognition system over two different chord sequences, one ordered naturally in \Cref{fig:in_order} and one ordered randomly in \Cref{fig:random}.
Both were generated from live performance.
We follow the graph convention described by \cite{harte}, where the 24 chords from C major to B minor are numbered 0 to 23, with no chord assigned 24, and we plot the number of each chord over time.


\subsection{Discussion}
After performing many tests using different chord sequences, we can properly evaluate the performance of our project.
We chose two particular tests to display due to some typical properties that they showcase.
The first test, in \Cref{fig:in_order}, shows correct recognition of about 80\% of the chords played.
The behavior in the other 20\% is notable, because it is due to playing a note off of the metronome beat.
This meant that the full audio frame of that chord was cut off and the program did not have enough audio of decent quality to properly perform chord recognition, so the chord was incorrectly recorded.
In most cases where the audio is out of sync with the metronome, the chord is classified as chord 24 or “no chord” as seen in \Cref{fig:in_order}.
In fewer cases, the chord is classified as the most similar chord, which is usually the minor or major chord in the same key as they share two out of three notes.

The behavior of the test shown in \Cref{fig:random} is more representative of the majority of our results.
Only two of the chords were incorrectly classified (about 96\% accuracy), and both of these chords were classified as “no chord.” This is likely due to the same situation as mentioned above, in which the frame was cut off.
We found that when the chord is played right on the metronome beat, it is nearly always classified correctly.
Only when the full audio frame is misrepresented due to being cut off, does the recognition algorithm fail.

Another reason that could potentially incur failure is improper calibration of the microphone.
Our low pass filter was needed to filter out noise inherent to our model of microphone.
It is possible that another microphone would need more or less aggressive filtering.
We used an average to lower-end microphone, so we estimate that few modifications would be needed to be made to the filter.
Use of a higher quality microphone would most likely not worsen performance.


\bibliographystyle{IEEEtran}
\bibliography{refs}


\onecolumn
\begin{lstlisting}[language = C, frame = single, caption = {An example code snippet from the chroma feature extraction containing the \texttt{process\_audio\_frame()} function.
This function is located in the \texttt{chromagram.c} file.}, captionpos = b, label = code:audio]
void process_audio_frame(int16_t* input_audio_frame)
{
    /*
     * Processes the audio input and generates the chromagram.
     *
     * First downsamples the input audio signal and calculates 
     * the magnitude of the FFT, then generates the chromagram.
     */

    chroma_ready = 0;

    downsampled_audio_frame_size = FRAME_SIZE / 4;
    downsampled_input_audio_frame = (float*)
    malloc(downsampled_audio_frame_size * sizeof(float));
    downsample_frame(input_audio_frame);

    int i;
    for (i = 0; i < BUFFER_SIZE; i++)
    {
        x_sp[2 * i] = downsampled_input_audio_frame[i];
        x_sp[2 * i + 1] = 0;
    }

    free(downsampled_input_audio_frame);
    calculate_chromagram();
}
\end{lstlisting}


\end{document}